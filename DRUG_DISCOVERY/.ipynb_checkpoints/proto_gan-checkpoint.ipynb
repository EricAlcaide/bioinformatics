{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AL', 'Na', 'Ba', 'Be', 'Mg', 'Ca', 'Br', 'Te', 'Se', 'Si', 'se', 'si', 'br', 'Li', 'Fe', 'Cl']\n"
     ]
    }
   ],
   "source": [
    "# GAN to determine validity of smiles\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "origin = \"data.txt\"\n",
    "vocab_list = [' ', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'n', '@', '-', 'AL', ')', '[', ']', 'Na', 'Ba', 'Be',\n",
    "\t\t \t  'Mg', 'Ca', 'K', 'Br', 'C', 'Te', 'I', 'S', 'Se', 'Si', 's', 'se', 'si', 'o', 'b', 'br', 'Li', 'c', 'B',\n",
    "\t\t \t  '.', '=', '+', 'N', 'F', 'Fe', 'Cl', '\\\\', '/', '\\t', '#', '(', 'l', '5', 'P', 'H', 'O']\n",
    "\n",
    "vocab_special = [c for c in vocab_list if len(c)>1]\n",
    "print(vocab_special)\n",
    "\n",
    "# Create vocab encoder and decoder\n",
    "vocab_enc = {}\n",
    "for i, el in enumerate(vocab_list):\n",
    "\tvocab_enc[el] = i\n",
    "vocab_dec = {}\n",
    "for i, el in enumerate(vocab_list):\n",
    "\tvocab_dec[i] = el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(filename):\n",
    "\t\"\"\" Extract data from the txt file.\"\"\"\n",
    "\tt = pd.read_csv('data.txt', header = None).values.tolist()\n",
    "\treturn [subs[0] for subs in t]\n",
    "\n",
    "data = extract(origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C[C@H](NCc1ccc(OCc2cccc(F)c2)cc1)C(=O)N', 'CCC(C1=C(O)Oc2ccccc2C1=O)c3ccccc3', 'C(=O)=O', 'O', 'OO']\n",
      "[' ', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'n', '@', '-', 'AL', ')', '[', ']', 'Na', 'Ba', 'Be', 'Mg', 'Ca', 'K', 'Br', 'C', 'Te', 'I', 'S', 'Se', 'Si', 's', 'se', 'si', 'o', 'b', 'br', 'Li', 'c', 'B', '.', '=', '+', 'N', 'F', 'Fe', 'Cl', '\\\\', '/', '\\t', '#', '(', 'l', '5', 'P', 'H', 'O']\n",
      "Total Compounds for training:  189\n",
      "Total Vocab:  56\n"
     ]
    }
   ],
   "source": [
    "print(data[:5])\n",
    "print(vocab_list)\n",
    "\n",
    "n_compounds = len(data)\n",
    "n_vocab = len(vocab_list)\n",
    "print(\"Total Compounds for training: \", n_compounds)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(compound):\n",
    "\tcode = []\n",
    "\twhile len(compound)>0:\n",
    "\t\tif compound[:2] in vocab_special:\n",
    "\t\t\tcode.append(vocab_enc[compound[:2]])\n",
    "\t\t\tcompound = compound[2:]\n",
    "\t\telse:\n",
    "\t\t\tcode.append(vocab_enc[compound[:1]])\n",
    "\t\t\tcompound = compound[1:]\n",
    "\treturn code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(data, seq_len=60): \n",
    "\t# measure max length - 60\n",
    "\t# seq_len = max([len(encode(subs)) for subs in data])\n",
    "\tdata = [encode(subs) for subs in data]\n",
    "\tdatex = []\n",
    "\tfor subs in data:\n",
    "\t\twhile len(subs)<60:\n",
    "\t\t\tsubs.append(0)\n",
    "\t\tdatex.append(subs)\n",
    "\treturn np.array(datex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READY DATA\n",
      "[[24 15 24 11 54 16 50 42 24 37  1 37 37 37 50 55 24 37  2 37 37 37 37 50\n",
      "  43 14 37  2 14 37 37  1 14 24 50 40 55 14 42  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [24 24 24 50 24  1 40 24 50 55 14 55 37  2 37 37 37 37 37  2 24  1 40 55\n",
      "  14 37  3 37 37 37 37 37  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [24 50 40 55 14 40 55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "(189, 60)\n"
     ]
    }
   ],
   "source": [
    "data_ready = data_prep(data)\n",
    "print(\"READY DATA\")\n",
    "print(data_ready[:3])\n",
    "print(data_ready.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42857143 0.26785714 0.42857143 0.19642857 0.96428571 0.28571429\n",
      "  0.89285714 0.75       0.42857143 0.66071429 0.01785714 0.66071429\n",
      "  0.66071429 0.66071429 0.89285714 0.98214286 0.42857143 0.66071429\n",
      "  0.03571429 0.66071429 0.66071429 0.66071429 0.66071429 0.89285714\n",
      "  0.76785714 0.25       0.66071429 0.03571429 0.25       0.66071429\n",
      "  0.66071429 0.01785714 0.25       0.42857143 0.89285714 0.71428571\n",
      "  0.98214286 0.25       0.75       0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# normalize data\n",
    "data_ready = data_ready/n_vocab\n",
    "print(data_ready[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 60)\n",
      "(189, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(data_ready, (data_ready.shape[0], data_ready.shape[1]))\n",
    "Y = np.ones((X.shape[0], 1))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WORKING GREAT TILL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\eric\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, GRU, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating discriminator\n",
    "embedding_vecor_length = 4\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Embedding(60*n_compounds, embedding_vecor_length, input_length=60))\n",
    "discriminator.add(LSTM(20))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Attempt to create custom loss and insert discriminator here but doesn't work\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_loss(y_true,y_pred):\n",
    "    return K.mean((discriminator.predict(y_pred) - y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = Sequential()\n",
    "generator.add(Embedding(60*n_compounds, 2, input_length=4))\n",
    "generator.add(LSTM(20))\n",
    "generator.add(Dense(60, activation='sigmoid'))\n",
    "generator.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dims of the initial noise dimension\n",
    "randomDim = 4\n",
    "# Combined network\n",
    "discriminator.trainable = False\n",
    "ganInput = Input(shape=(randomDim,))\n",
    "x = generator(ganInput)\n",
    "ganOutput = discriminator(x)\n",
    "gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 4, 2)              22680     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 20)                1840      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 60)                1260      \n",
      "=================================================================\n",
      "Total params: 25,780\n",
      "Trainable params: 25,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 60, 4)             45360     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 20)                2000      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 47,381\n",
      "Trainable params: 0\n",
      "Non-trainable params: 47,381\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (None, 60)                25780     \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    (None, 1)                 47381     \n",
      "=================================================================\n",
      "Total params: 73,161\n",
      "Trainable params: 25,780\n",
      "Non-trainable params: 47,381\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()\n",
    "discriminator.summary()\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49934748],\n",
       "       [0.49934748]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.predict(np.random.normal(0,1, size=[2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generator.predict(np.random.normal(0,1, size=[2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49934748],\n",
       "       [0.49934748],\n",
       "       [0.49934748],\n",
       "       [0.49934748],\n",
       "       [0.49934748]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.predict(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(generator.trainable)\n",
    "print(discriminator.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, epochs=2, batchSize=8, randomDim=randomDim):\n",
    "    batchCount = n_compounds / batchSize\n",
    "    print('Epochs:', epochs)\n",
    "    print('Batch size:', batchSize)\n",
    "    print('Batches per epoch:', batchCount)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "        for _ in tqdm(range(int(batchCount))):\n",
    "            # Get a random set of input noise and images\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "            imageBatch = X[np.random.randint(0, X.shape[0], size=batchSize)]\n",
    "\n",
    "            # Generate fake MNIST images\n",
    "            generatedImages = generator.predict(noise)\n",
    "            # print np.shape(imageBatch), np.shape(generatedImages)\n",
    "            X_full = np.concatenate([imageBatch, generatedImages])\n",
    "\n",
    "            # Labels for generated and real data\n",
    "            yDis = np.zeros(2*batchSize)\n",
    "            # One-sided label smoothing\n",
    "            yDis[:batchSize] = 0.9\n",
    "\n",
    "            # Train discriminator\n",
    "            discriminator.trainable = True\n",
    "            dloss = discriminator.train_on_batch(X_full, yDis)\n",
    "\n",
    "            # Train generator\n",
    "            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n",
    "            yGen = np.ones(batchSize)\n",
    "            discriminator.trainable = False\n",
    "            print(noise, yGen)\n",
    "            gloss = gan.train_on_batch(noise, yGen)\n",
    "\n",
    "        # Store loss of most recent batch from this epoch\n",
    "        print(\"Discriminator Loss:\", dloss)\n",
    "        print(\"Generator Loss:\", gloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a wall of generated MNIST images\n",
    "def printGeneratedSubs(epoch, examples=5, dim=(10, 10), figsize=(10, 10)):\n",
    "    noise = np.random.normal(0, 1, size=[examples, randomDim])\n",
    "    generatedImages = generator.predict(noise)\n",
    "    generatedImages = generatedImages.reshape(examples, 60)\n",
    "\n",
    "    for subs in generatedImages:\n",
    "        smiles = \"\"\n",
    "        for element in subs:\n",
    "            smiles = smiles + vocab_dec[element]\n",
    "        print(smiles)\n",
    "\n",
    "# Save the generator and discriminator networks (and weights) for later use\n",
    "def saveModels(epoch):\n",
    "    generator.save('gan_generator.h5')\n",
    "    discriminator.save('gan_discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2\n",
      "Batch size: 8\n",
      "Batches per epoch: 23.625\n",
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-43e820ada943>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-8b681ccb93f9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, epochs, batchSize, randomDim)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mgeneratedImages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;31m# print np.shape(imageBatch), np.shape(generatedImages)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mX_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimageBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeneratedImages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Labels for generated and real data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "train(X, epochs=2, batchSize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
